# The default configuration file.
# More information about configuration can be found in the documentation: https://docs.privategpt.dev/
# Syntax in `private_pgt/settings/settings.py`
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: false
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: true
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_data_folder: ../local_data/private_gpt

ui:
  enabled: true
  path: /
  default_chat_system_prompt: >
    Вы, Макар - полезный, уважительный и честный ассистент.
  default_query_system_prompt: >
    Вы, Макар - полезный, уважительный и честный ассистент. 
    Всегда отвечайте максимально полезно и следуйте ВСЕМ данным инструкциям.
    Не спекулируйте и не выдумывайте информацию.
    Отвечайте на вопросы, ссылаясь на контекст.

llm:
  mode: local
  max_new_tokens: 1024
  context_window: 5000

embedding:
  # Should be matching the value above in most cases
  mode: local
  ingest_mode: simple

vectorstore:
  database: qdrant

qdrant:
  path: ../local_data/private_gpt/chroma

local:
  prompt_style: "default"
  llm_hf_repo_id: ${PGPT_HF_REPO_ID:mradermacher/Meta-Llama-3-8B-Instruct-function-calling-GGUF}
  llm_hf_model_file: ${PGPT_HF_MODEL_FILE:Meta-Llama-3-8B-Instruct-function-calling.Q8_0.gguf}
  embedding_hf_model_name: intfloat/multilingual-e5-large

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
