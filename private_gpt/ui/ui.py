"""This file should be imported only and only if you want to run the UI locally."""
import itertools
import time
import logging
import os.path
import threading
from collections.abc import Iterable
from pathlib import Path
from typing import Any, List

import gradio as gr  # type: ignore
from fastapi import FastAPI
from gradio.themes.utils.colors import slate  # type: ignore
from injector import inject, singleton
from llama_index.llms import ChatMessage, ChatResponse, MessageRole
from pydantic import BaseModel

from private_gpt.constants import PROJECT_ROOT_PATH
from private_gpt.di import global_injector
from private_gpt.server.chat.chat_service import ChatService, CompletionGen
from private_gpt.server.chunks.chunks_service import Chunk, ChunksService
from private_gpt.server.ingest.ingest_service import IngestService
from private_gpt.settings.settings import settings
from private_gpt.ui.images import FAVICON_PATH

import re
import tempfile

logger = logging.getLogger(__name__)

THIS_DIRECTORY_RELATIVE = Path(__file__).parent.relative_to(PROJECT_ROOT_PATH)
# Should be "private_gpt/ui/avatar-bot.ico"
AVATAR_USER = THIS_DIRECTORY_RELATIVE / "icons8-—á–µ–ª–æ–≤–µ–∫-96.png"
AVATAR_BOT = THIS_DIRECTORY_RELATIVE / "icons8-bot-96.png"

UI_TAB_TITLE = "MakarGPT"

SOURCES_SEPARATOR = "\n\n –î–æ–∫—É–º–µ–Ω—Ç—ã: \n"

MODES = ["DB", "LLM"]
MAX_NEW_TOKENS: int = 1500
LINEBREAK_TOKEN: int = 13
SYSTEM_TOKEN: int = 1788
USER_TOKEN: int = 1404
BOT_TOKEN: int = 9225
COUNT_THREAD = 2

ROLE_TOKENS: dict = {
    "user": USER_TOKEN,
    "bot": BOT_TOKEN,
    "system": SYSTEM_TOKEN
}

BLOCK_CSS = """

#buttons button {
    min-width: min(120px,100%);
}

/* –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª–∏ –¥–ª—è td */
tr focus {
  user-select: all; /* –†–∞–∑—Ä–µ—à–∞–µ–º –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ */
}

/* –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª–∏ –¥–ª—è —è—á–µ–π–∫–∏ span –≤–Ω—É—Ç—Ä–∏ td */
tr span {
  user-select: all; /* –†–∞–∑—Ä–µ—à–∞–µ–º –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ */
}

"""


class Source(BaseModel):
    file: str
    page: str
    text: str

    class Config:
        frozen = True

    @staticmethod
    def curate_sources(sources: list[Chunk]) -> set["Source"]:
        curated_sources = set()

        for chunk in sources:
            doc_metadata = chunk.document.doc_metadata

            file_name = os.path.basename(doc_metadata.get("file_name", "-")) if doc_metadata else "-"
            page_label = doc_metadata.get("page_label", "-") if doc_metadata else "-"

            source = Source(file=file_name, page=page_label, text=chunk.text)
            curated_sources.add(source)

        return curated_sources


@singleton
class PrivateGptUi:
    @inject
    def __init__(
        self,
        ingest_service: IngestService,
        chat_service: ChatService,
        chunks_service: ChunksService,
    ) -> None:
        self._ingest_service = ingest_service
        self._chat_service = chat_service
        self._chunks_service = chunks_service

        # Cache the UI blocks
        self._ui_block = None

        # self.semaphore = threading.Semaphore(COUNT_THREAD)

        # Initialize system prompt based on default mode
        self.mode = MODES[0]
        self._system_prompt = self._get_default_system_prompt(self.mode)

    def _get_context(self, history: list[list[str]], mode: str, limit, *_: Any):
        match mode:
            case "DB":
                content = self._chat_service.retrieve(
                    history=history,
                    use_context=True,
                    limit=limit
                )
                return content, mode
            case "LLM":
                content = self._chat_service.retrieve(
                    history=history,
                    use_context=False,
                )
                return content, mode

    # On initialization and on mode change, this function set the system prompt
    # to the default prompt based on the mode (and user settings).
    @staticmethod
    def _get_default_system_prompt(mode: str) -> str:
        p = ""
        match mode:
            # For query chat mode, obtain default system prompt from settings
            case "DB":
                p = settings().ui.default_query_system_prompt
            # For chat mode, obtain default system prompt from settings
            case "LLM":
                p = settings().ui.default_chat_system_prompt
            # For any other mode, clear the system prompt
            case _:
                p = ""
        return p

    def _set_system_prompt(self, system_prompt_input: str) -> None:
        logger.info(f"Setting system prompt to: {system_prompt_input}")
        self._system_prompt = system_prompt_input

    def _set_current_mode(self, mode: str) -> Any:
        self.mode = mode
        self._set_system_prompt(self._get_default_system_prompt(mode))
        # Update placeholder and allow interaction if default system prompt is set
        if self._system_prompt:
            return gr.update(placeholder=self._system_prompt, interactive=True)
        # Update placeholder and disable interaction if no default system prompt is set
        else:
            return gr.update(placeholder=self._system_prompt, interactive=False)

    def _list_ingested_files(self):
        return self._ingest_service.list_ingested_langchain()

    def delete_doc(self, documents: str):
        logger.info(f"Documents is {documents}")
        list_documents: list[str] = documents.strip().split("\n")
        for node in self._ingest_service.list_ingested_langchain():
            if node.doc_id is not None and os.path.basename(node.doc_metadata["file_name"]) in list_documents:
                self._ingest_service.delete(node.doc_id)
        return "", self._list_ingested_files()

    def user(self, message, history):
        logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞")
        # self.semaphore.acquire()
        if history is None:
            history = []
        new_history = history + [[message, None]]
        # self.semaphore.release()
        logger.info("–ó–∞–∫–æ–Ω—á–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞")
        return "", new_history

    @staticmethod
    def regenerate_response(history):
        """

        :param history:
        :return:
        """
        return "", history

    @staticmethod
    def get_message_tokens(model, role: str, content: str) -> list:
        """

        :param model:
        :param role:
        :param content:
        :return:
        """
        message_tokens: list = model.tokenize(content.encode("utf-8"))
        message_tokens.insert(1, ROLE_TOKENS[role])
        message_tokens.insert(2, LINEBREAK_TOKEN)
        message_tokens.append(model.token_eos())
        return message_tokens

    def get_system_tokens(self, model) -> list:
        """

        :param model:
        :return:
        """
        system_message: dict = {"role": "system", "content": self._system_prompt}
        return self.get_message_tokens(model, **system_message)

    def bot(self, history, retrieved_docs, mode):
        """

        :param history:
        :param retrieved_docs:
        :param mode:
        :return:
        """
        logger.info("–ü–æ–ª—É—á–∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞")
        # self.semaphore.acquire()
        if not history or not history[-1][0]:
            yield history[:-1]
            return
        model = self._chat_service.llm
        tokens = self.get_system_tokens(model)[:]
        tokens.append(LINEBREAK_TOKEN)

        for user_message, bot_message in history[-4:-1]:
            message_tokens = self.get_message_tokens(model=model, role="user", content=user_message)
            tokens.extend(message_tokens)

        last_user_message = history[-1][0]
        pattern = r'<a\s+[^>]*>(.*?)</a>'
        files = re.findall(pattern, retrieved_docs)
        for file in files:
            retrieved_docs = re.sub(fr'<a\s+[^>]*>{file}</a>', file, retrieved_docs)
        if retrieved_docs and mode:
            last_user_message = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {retrieved_docs}\n\n–ò—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: " \
                                f"{last_user_message}"
        message_tokens = self.get_message_tokens(model=model, role="user", content=last_user_message)
        tokens.extend(message_tokens)

        role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]
        tokens.extend(role_tokens)
        generator = model.generate(
            tokens,
            top_k=80,
            top_p=0.9,
            temp=0.1
        )

        partial_text = ""
        for i, token in enumerate(generator):
            if token == model.token_eos() or (MAX_NEW_TOKENS is not None and i >= MAX_NEW_TOKENS):
                break
            partial_text += model.detokenize([token]).decode("utf-8", "ignore")
            history[-1][1] = partial_text
            yield history

        if files:
            partial_text += SOURCES_SEPARATOR
            sources_text = "\n\n\n".join(
                f"{index}. {source}"
                for index, source in enumerate(files, start=1)
            )
            partial_text += sources_text
            history[-1][1] = partial_text
        yield history
        # self.semaphore.release()

    def _chat(self, history, context, mode):
        match mode:
            case "DB":
                yield from self.bot(history, context, True)
            case "LLM":
                yield from self.bot(history, context, False)

    def _upload_file(self, files: List[tempfile.TemporaryFile], chunk_size: int, chunk_overlap: int):
        logger.debug("Loading count=%s files", len(files))
        message = self._ingest_service.bulk_ingest([f.name for f in files], chunk_size, chunk_overlap)
        return message, self._list_ingested_files()

    def _build_ui_blocks(self) -> gr.Blocks:
        logger.debug("Creating the UI blocks")
        with gr.Blocks(
            title=UI_TAB_TITLE,
            theme=gr.themes.Soft(),
            css=BLOCK_CSS
        ) as blocks:
            logo_svg = f'<img src="{FAVICON_PATH}" width="48px" style="display: inline">'
            gr.Markdown(
                f"""<h1><center>{logo_svg} –Ø, –ú–∞–∫–∞—Ä - –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –†—É—Å–∫–æ–Ω</center></h1>"""
            )

            with gr.Tab("–ß–∞—Ç"):
                with gr.Accordion("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã", open=False):
                    with gr.Tab(label="–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"):
                        limit = gr.Slider(
                            minimum=1,
                            maximum=5,
                            value=4,
                            step=1,
                            interactive=True,
                            label="–ö–æ–ª-–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
                        )
                    with gr.Tab(label="–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Ä–µ–∑–∫–∏"):
                        chunk_size = gr.Slider(
                            minimum=50,
                            maximum=1024,
                            value=1024,
                            step=128,
                            interactive=True,
                            label="–†–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤",
                        )
                        chunk_overlap = gr.Slider(
                            minimum=0,
                            maximum=500,
                            value=100,
                            step=10,
                            interactive=True,
                            label="–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ"
                        )

                with gr.Accordion("–ö–æ–Ω—Ç–µ–∫—Å—Ç", open=False):
                    with gr.Column(variant="compact"):
                        content = gr.Markdown(
                            value="–ü–æ—è–≤—è—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–¥–∞–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤",
                            label="–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã",
                            show_label=True
                        )

                with gr.Row():
                    with gr.Column(scale=4, variant="compact"):
                        mode = gr.Radio(
                            MODES,
                            label="–ö–æ–ª–ª–µ–∫—Ü–∏–∏",
                            value="DB",
                            info="–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—ã–±–æ—Ä–æ–º –∫–æ–ª–ª–µ–∫—Ü–∏–π. –ù—É–∂–µ–Ω –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–ª–∏ –Ω–µ—Ç?"
                        )
                        upload_button = gr.Files(
                            file_count="multiple"
                        )
                        file_warning = gr.Markdown("–§—Ä–∞–≥–º–µ–Ω—Ç—ã –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")

                    with gr.Column(scale=10):
                        chatbot = gr.Chatbot(
                            label="–î–∏–∞–ª–æ–≥",
                            height=500,
                            show_copy_button=True,
                            show_share_button=True,
                            avatar_images=(
                                AVATAR_USER,
                                AVATAR_BOT
                            )
                        )
                        with gr.Accordion("–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç", open=False):
                            system_prompt_input = gr.Textbox(
                                placeholder=self._system_prompt,
                                label="–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç",
                                lines=2
                            )
                            # On blur, set system prompt to use in queries
                            system_prompt_input.blur(
                                self._set_system_prompt,
                                inputs=system_prompt_input,
                            )

                with gr.Row():
                    with gr.Column(scale=20):
                        msg = gr.Textbox(
                            label="–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ",
                            show_label=False,
                            placeholder="üëâ –ù–∞–ø–∏—à–∏—Ç–µ –∑–∞–ø—Ä–æ—Å",
                            container=False
                        )
                    with gr.Column(scale=3, min_width=100):
                        submit = gr.Button("üì§ –û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary")

                with gr.Row(elem_id="buttons"):
                    gr.Button(value="üëç –ü–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å")
                    gr.Button(value="üëé –ù–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å")
                    stop = gr.Button(value="‚õî –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å")
                    regenerate = gr.Button(value="üîÑ –ü–æ–≤—Ç–æ—Ä–∏—Ç—å")
                    clear = gr.Button(value="üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å")

                with gr.Row():
                    gr.Markdown(
                        "<center>–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç –¥–æ–ø—É—Å–∫–∞—Ç—å –æ—à–∏–±–∫–∏, –ø–æ—ç—Ç–æ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. "
                        "–û—Ç–≤–µ—Ç—ã —Ç–∞–∫–∂–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏–∑—ã–≤–æ–º –∫ –¥–µ–π—Å—Ç–≤–∏—é</center>"
                    )

            with gr.Tab("–î–æ–∫—É–º–µ–Ω—Ç—ã"):
                with gr.Row():
                    with gr.Column(scale=3):
                        find_doc = gr.Textbox(
                            label="–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ",
                            show_label=False,
                            placeholder="üëâ –ù–∞–ø–∏—à–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                            container=False
                        )
                        delete = gr.Button("üßπ –£–¥–∞–ª–∏—Ç—å", variant="primary")
                    with gr.Column(scale=7):
                        ingested_dataset = gr.List(
                            self._list_ingested_files,
                            headers=["–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤"],
                            interactive=False,
                            render=False,  # Rendered under the button
                        )
                        ingested_dataset.change(
                            self._list_ingested_files,
                            outputs=ingested_dataset,
                        )
                        ingested_dataset.render()

            mode.change(
                self._set_current_mode, inputs=mode, outputs=system_prompt_input
            )

            upload_button.upload(
                self._upload_file,
                inputs=[upload_button, chunk_size, chunk_overlap],
                outputs=[file_warning, ingested_dataset],
            )

            # Delete documents from db
            delete.click(
                fn=self.delete_doc,
                inputs=find_doc,
                outputs=[find_doc, ingested_dataset]
            )

            # Pressing Enter
            submit_event = msg.submit(
                fn=self.user,
                inputs=[msg, chatbot],
                outputs=[msg, chatbot],
                concurrency_limit=COUNT_THREAD
            ).success(
                fn=self._get_context,
                inputs=[chatbot, mode, limit],
                outputs=[content, mode],
                concurrency_limit=COUNT_THREAD
            ).success(
                fn=self._chat,
                inputs=[chatbot, content, mode],
                outputs=chatbot,
                concurrency_limit=COUNT_THREAD
            )

            # Pressing the button
            submit_click_event = submit.click(
                fn=self.user,
                inputs=[msg, chatbot],
                outputs=[msg, chatbot],
                concurrency_limit=COUNT_THREAD
            ).success(
                fn=self._get_context,
                inputs=[chatbot, mode, limit],
                outputs=[content, mode],
                concurrency_limit=COUNT_THREAD
            ).success(
                fn=self._chat,
                inputs=[chatbot, content, mode],
                outputs=chatbot,
                concurrency_limit=COUNT_THREAD,
            )

            # Regenerate
            regenerate_click_event = regenerate.click(
                fn=self.regenerate_response,
                inputs=[chatbot],
                outputs=[msg, chatbot],
                concurrency_limit=COUNT_THREAD
            ).success(
                fn=self._get_context,
                inputs=[chatbot, mode, limit],
                outputs=[content, mode],
                concurrency_limit=COUNT_THREAD
            ).success(
                fn=self._chat,
                inputs=[chatbot, content, mode],
                outputs=chatbot,
                concurrency_limit=COUNT_THREAD
            )

            # Stop generation
            stop.click(
                fn=None,
                inputs=None,
                outputs=None,
                cancels=[submit_event, submit_click_event, regenerate_click_event],
                queue=False,
            )

            # Clear history
            clear.click(lambda: None, None, chatbot, queue=False)

        return blocks

    def get_ui_blocks(self) -> gr.Blocks:
        if self._ui_block is None:
            self._ui_block = self._build_ui_blocks()
        return self._ui_block

    def mount_in_app(self, app: FastAPI, path: str) -> None:
        blocks = self.get_ui_blocks()
        blocks.queue()
        logger.info("Mounting the gradio UI, at path=%s", path)
        gr.mount_gradio_app(app, blocks, path=path)


if __name__ == "__main__":
    ui = global_injector.get(PrivateGptUi)
    _blocks = ui.get_ui_blocks()
    _blocks.queue(default_concurrency_limit=COUNT_THREAD)
    _blocks.launch(debug=False, show_api=False)
